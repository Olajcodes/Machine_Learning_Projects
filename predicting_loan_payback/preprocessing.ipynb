{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f544f1",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "001c716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfile import ZipFile\n",
    "import joblib\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, power_transform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58c25d",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6500310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "sample_submission.csv                          2025-10-28 23:08:48      2291139\n",
      "test.csv                                       2025-10-28 23:08:48     23021430\n",
      "train.csv                                      2025-10-28 23:08:50     55988519\n"
     ]
    }
   ],
   "source": [
    "zip_path = r\"C:\\Users\\Welcome Sir\\Downloads\\playground-series-s5e11.zip\"\n",
    "\n",
    "with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb6f665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train data successfully loaded...\n",
      "\n",
      "Test data successfully loaded...\n"
     ]
    }
   ],
   "source": [
    "# Loading the \"test\" and \"train\" data\n",
    "\n",
    "with ZipFile(zip_path) as z:\n",
    "    with z.open('train.csv') as f:\n",
    "        train_data = pd.read_csv(f)\n",
    "        print(\"\\nTrain data successfully loaded...\")\n",
    "        \n",
    "    with z.open('test.csv') as f:\n",
    "        test_data = pd.read_csv(f)\n",
    "        print(\"\\nTest data successfully loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9db635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the data\n",
    "train_df = train_data.copy()\n",
    "test_df = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a48e72",
   "metadata": {},
   "source": [
    "**From the EDA, `Power transformation` from recommended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72389a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split our data\n",
    "X = train_df.drop(columns=['loan_paid_back', 'id'])\n",
    "y = train_df['loan_paid_back']\n",
    "\n",
    "# Let's do the same to the test data\n",
    "test_df = test_df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2080282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce the skewness of the numerical columns using power transformation\n",
    "def handle_skewness(df, columns:dict):\n",
    "    df = df.copy()\n",
    "    df[columns] = power_transform(df[columns], method='yeo-johnson')\n",
    "    return df\n",
    "\n",
    "# Let's treat the skewness of numerical in X\n",
    "num_col = X.select_dtypes(include='number').columns\n",
    "X = handle_skewness(X, num_col)\n",
    "\n",
    "\n",
    "# Let's do same for the test data as well.\n",
    "test_num_col = test_df.select_dtypes(include='number').columns\n",
    "test_df = handle_skewness(test_df, test_num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5b251",
   "metadata": {},
   "source": [
    "**Encoding Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20363169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'marital_status', 'education_level', 'employment_status',\n",
       "       'loan_purpose', 'grade_subgrade'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col = train_df.select_dtypes(include='object').columns\n",
    "cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "513176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding categorical columns\n",
    "def one_hot_encoder(df, columns:dict):\n",
    "    df = df.copy()\n",
    "    df = pd.get_dummies(df, columns=columns, drop_first=True)\n",
    "    return df\n",
    "\n",
    "# based on EDA column to be encoded [gender,marital_status,employment_status]\n",
    "cat=['gender','marital_status','employment_status','loan_purpose']\n",
    "X= one_hot_encoder(X, cat)\n",
    "\n",
    "# let one hot encode the categorical columns in test data too\n",
    "test_data= one_hot_encoder(test_data, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a772f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual encoding target column [education_level,grade_subgrade]\n",
    "def manual_encoder(df, column:str, mapping:dict):\n",
    "    df=df.copy()\n",
    "    df[column]= df[column].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1ebc8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding education_level\n",
    "edu_order = {\n",
    "    \"High School\": 1,\n",
    "    \"Bachelor's\": 2,\n",
    "    \"Master's\": 3,\n",
    "    \"PhD\": 4,\n",
    "    \"Other\": 0\n",
    "}\n",
    "X= manual_encoder(X, 'education_level', edu_order)\n",
    "\n",
    "# let manual encode education_level in test data too\n",
    "test_data= manual_encoder(test_data, 'education_level', edu_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c55c9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding grade_subgrade\n",
    "grade_order = [\n",
    "    'A1','A2','A3','A4','A5',\n",
    "    'B1','B2','B3','B4','B5',\n",
    "    'C1','C2','C3','C4','C5',\n",
    "    'D1','D2','D3','D4','D5',\n",
    "    'E1','E2','E3','E4','E5',\n",
    "    'F1','F2','F3','F4','F5']\n",
    "\n",
    "grade_mapping= {grade: idx+1 for idx, grade in enumerate(grade_order)}\n",
    "X= manual_encoder(X, 'grade_subgrade', grade_mapping)\n",
    "\n",
    "# let manual encode grade_subgrade in test data too\n",
    "test_data= manual_encoder(test_data, 'grade_subgrade', grade_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a5baa",
   "metadata": {},
   "source": [
    "**Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15d8a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let scale the numerical columns using minmax scaler \n",
    "scaler= MinMaxScaler()\n",
    "X[num_col]= scaler.fit_transform(X[num_col])\n",
    "\n",
    "# let scale the numerical columns in test data too\n",
    "test_data[num_col]= scaler.transform(test_data[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b2c154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved -> artifacts/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# To save the scaler for future usage\n",
    "import os\n",
    "os.makedirs(\"artifacts\", exist_ok = True)\n",
    "joblib.dump(scaler, \"artifacts/scaler.pkl\")\n",
    "\n",
    "print(\"Scaler saved -> artifacts/scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292b855",
   "metadata": {},
   "source": [
    "**Splitting the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4d48775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_scaled= X[num_col]\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6078d17",
   "metadata": {},
   "source": [
    "**Building and comparing Multiple Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f25424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=234),\n",
    "#     \"Random Forest\": RandomForestClassifier(random_state=234),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(random_state=234),\n",
    "#     \"KNN\": KNeighborsClassifier(),\n",
    "#     \"CatBoost\": CatBoostClassifier()\n",
    "# }\n",
    "\n",
    "# # Initialize a dictionary to store the models\n",
    "# results = {}\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_val)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "#     results[model_name] = accuracy\n",
    "#     print(f\"{model_name} Accuracy:{accuracy:.3f}\")\n",
    "    \n",
    "#     # Let's include confusion matrix\n",
    "#     cm = confusion_matrix(y_val, y_pred)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    \n",
    "#     # Plot with model name as title\n",
    "#     disp.plot(cmap='Blues')\n",
    "#     plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "#     plt.show()\n",
    "    \n",
    "# # Let print the results in a dataframe for better visualization\n",
    "# results_df= pd.DataFrame(results.items(), columns=['Model', 'Accuracy'])\n",
    "# results_df= results_df.sort_values(by='Accuracy', ascending=False)\n",
    "# results_df.reset_index(drop=True, inplace=True)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff9e00",
   "metadata": {},
   "source": [
    "**Optimize Model Using Randomized Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23fd95",
   "metadata": {},
   "source": [
    "Since `CatBoost` is the highest with accuracy and less `type 1 & 2`, I will tune `CatBoost` for stability and interpretability of  feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14ecdfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'colsample_bylevel': np.float64(0.8795229657109607), 'depth': 6, 'iterations': 599, 'l2_leaf_reg': np.float64(7.395218471109376), 'learning_rate': np.float64(0.10136872967685903), 'subsample': np.float64(0.7995179007653619)}\n",
      "Best F1 Score: 0.8985\n"
     ]
    }
   ],
   "source": [
    "# let tune the best trained Model using Random search\n",
    "cb = CatBoostClassifier(random_state=234, verbose=0)\n",
    "\n",
    "# parameter random for tuning\n",
    "# new changes\n",
    "param_dist = {\n",
    "    \"iterations\": randint(100, 600),\n",
    "    \"depth\": randint(3, 12),\n",
    "    \"learning_rate\": uniform(0.01, 0.2),\n",
    "    \"l2_leaf_reg\": uniform(1, 10),\n",
    "    \"subsample\": uniform(0.6, 0.4),\n",
    "    \"colsample_bylevel\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "# new Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cb,\n",
    "    param_distributions= param_dist,\n",
    "    n_iter=2, \n",
    "    cv=2, \n",
    "    scoring=\"f1\",\n",
    "    random_state=234,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "# Fit on your training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best F1 Score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# save the best model\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c05a77a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved successfully -> Artifacts.\n"
     ]
    }
   ],
   "source": [
    "# To save the best model\n",
    "import os\n",
    "os.makedirs(\"artifacts\", exist_ok = True)\n",
    "joblib.dump(best_model, 'artifacts/best_model.pkl')\n",
    "print(\"The best model has been saved successfully -> Artifacts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
